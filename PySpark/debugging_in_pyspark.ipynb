{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMJR5o6xx9IHwPUFOMVjVNP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zwelshman/healthcare-data-analysis-in-python/blob/main/PySpark/debugging_in_pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Debugging in PySpark"
      ],
      "metadata": {
        "id": "Y9_RhRYHMvXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "4wvZeTZbDbKW",
        "outputId": "0869cbec-6fa6-47d7-8c3c-a7fb1c378cfa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x781fe0151b70>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://8ff4bd61dc14:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2kX68VEUAKXd"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "def generate_fake_data():\n",
        "  \"\"\"\n",
        "  Simple function that generates fake data\n",
        "  and returns as a Spark DataFrame\n",
        "  \"\"\"\n",
        "  df = (\n",
        "      spark.createDataFrame(\n",
        "          [\n",
        "              (\"id_001\", \"2020-01-01\", 52),\n",
        "              (\"id_002\", \"2021-06-23\", 63),\n",
        "              (\"id_003\", \"2020-05-01\", 16)\n",
        "          ],\n",
        "          ['person_id', 'date', 'age']\n",
        "      )\n",
        "      .withColumn('date', F.to_date(F.col('date')))\n",
        "  )\n",
        "\n",
        "  return df\n",
        "\n",
        "df = generate_fake_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "id": "eTyNy8ViEXVl",
        "outputId": "02291fe3-f1a2-443c-8e5c-12d83e27c969"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "+---------+----------+---+\n",
              "|person_id|      date|age|\n",
              "+---------+----------+---+\n",
              "|   id_001|2020-01-01| 52|\n",
              "|   id_002|2021-06-23| 63|\n",
              "|   id_003|2020-05-01| 16|\n",
              "+---------+----------+---+"
            ],
            "text/html": [
              "<table border='1'>\n",
              "<tr><th>person_id</th><th>date</th><th>age</th></tr>\n",
              "<tr><td>id_001</td><td>2020-01-01</td><td>52</td></tr>\n",
              "<tr><td>id_002</td><td>2021-06-23</td><td>63</td></tr>\n",
              "<tr><td>id_003</td><td>2020-05-01</td><td>16</td></tr>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_days(df, date_col, num_days):\n",
        "  \"\"\"\n",
        "  Function to create a new column with the number of days\n",
        "  added to an original date column\n",
        "  \"\"\"\n",
        "\n",
        "  df = (df\n",
        "      .withColumn(f'DATE_PLUS{str(num_days)}',\n",
        "                  F.date_add(df[f'{date_col}'], num_days))\n",
        "  )\n",
        "\n",
        "  return df\n",
        "\n",
        "df_days = add_days(df, 'date', 10)\n",
        "\n",
        "display(df_days)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "id": "9DWAC6TiGKNy",
        "outputId": "71ff822c-36e5-4b16-867e-3c87cc12c88d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "+---------+----------+---+-----------+\n",
              "|person_id|      date|age|DATE_PLUS10|\n",
              "+---------+----------+---+-----------+\n",
              "|   id_001|2020-01-01| 52| 2020-01-11|\n",
              "|   id_002|2021-06-23| 63| 2021-07-03|\n",
              "|   id_003|2020-05-01| 16| 2020-05-11|\n",
              "+---------+----------+---+-----------+"
            ],
            "text/html": [
              "<table border='1'>\n",
              "<tr><th>person_id</th><th>date</th><th>age</th><th>DATE_PLUS10</th></tr>\n",
              "<tr><td>id_001</td><td>2020-01-01</td><td>52</td><td>2020-01-11</td></tr>\n",
              "<tr><td>id_002</td><td>2021-06-23</td><td>63</td><td>2021-07-03</td></tr>\n",
              "<tr><td>id_003</td><td>2020-05-01</td><td>16</td><td>2020-05-11</td></tr>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Debugging: Simple example\n",
        "\n",
        "There are two ways to entering debug mode:\n",
        "\n",
        "- Create a cell beneath the one with the error and type `%debug` or,\n",
        "- At the top of a notebook automatically turn on the debugger with`%pdb on` where pdb stands for the python debugger."
      ],
      "metadata": {
        "id": "4nbOBg6SNO2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might already know what the problem is in the code below, however with more complex code and functions it might not be as obvious."
      ],
      "metadata": {
        "id": "MWxy84tpOKJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_days_error = add_days(df, 'date_1', 10)\n",
        "\n",
        "display(df_days_error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "nQLmGxCUG9PX",
        "outputId": "88523fd5-9142-49e4-d6c4-79d44e0db98c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "Cannot resolve column name \"date_1\" among (person_id, date, age)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-556758be120f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_days_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_days\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'date_1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_days_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-9c63975990bf>\u001b[0m in \u001b[0;36madd_days\u001b[0;34m(df, date_col, num_days)\u001b[0m\n\u001b[1;32m      7\u001b[0m   df = (df\n\u001b[1;32m      8\u001b[0m       .withColumn(f'DATE_PLUS{str(num_days)}',\n\u001b[0;32m----> 9\u001b[0;31m                   F.date_add(df[f'{date_col}'], num_days))\n\u001b[0m\u001b[1;32m     10\u001b[0m   )\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1618\u001b[0m         \"\"\"\n\u001b[1;32m   1619\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m             \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Cannot resolve column name \"date_1\" among (person_id, date, age)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ```AnalysisExeception``` provides enough verbose to determine where the issues is, however we will ether the debug more to inspect.\n",
        "\n",
        "- We will use `%debug` to enter debug mode and look at the `converted` variable that contains the exception error.\n",
        "- Then we will jump up the stack trace, using the `u`  meaning up in the python debugger syntax, followed by pressing the return key, until we see the line of code where the exception started. (three jumps in this case).\n",
        "- Using the pdb interactive shell look at the spark DataDrame called `df` that was fed into the function.\n",
        "- The `add_days()` function can be accessed using `!help(add_days)`\n",
        "- Using `a` to see the arguments fed into the `add_days()` function we can see the `date_col` variable to be 'date_1' to invoke the error.\n",
        "- Rerun the add_days() function and set `date_col` variable to `'date'` using `date_col = 'date'`, like so `add_days(df,'date',20)`"
      ],
      "metadata": {
        "id": "bn7BoLl_OPyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%debug"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOnOhAGmBepG",
        "outputId": "72e0ce7e-09b3-41c3-f9e6-cde19342a3ea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> \u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m(117)\u001b[0;36mdeco\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    115 \u001b[0;31m                \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    116 \u001b[0;31m                \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 117 \u001b[0;31m                \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    118 \u001b[0;31m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    119 \u001b[0;31m                \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> u 3\n",
            "> \u001b[0;32m<ipython-input-4-9c63975990bf>\u001b[0m(9)\u001b[0;36madd_days\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m      7 \u001b[0;31m  df = (df\n",
            "\u001b[0m\u001b[0;32m      8 \u001b[0;31m      .withColumn(f'DATE_PLUS{str(num_days)}',\n",
            "\u001b[0m\u001b[0;32m----> 9 \u001b[0;31m                  F.date_add(df[f'{date_col}'], num_days))\n",
            "\u001b[0m\u001b[0;32m     10 \u001b[0;31m  )\n",
            "\u001b[0m\u001b[0;32m     11 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> df\n",
            "DataFrame[person_id: string, date: date, age: bigint]\n",
            "ipdb> !help(add_days)\n",
            "Help on function add_days in module __main__:\n",
            "\n",
            "add_days(df, date_col, num_days)\n",
            "    Function to create a new column with the number of days\n",
            "    added to an original date column\n",
            "\n",
            "ipdb> a\n",
            "df = DataFrame[person_id: string, date: date, age: bigint]\n",
            "date_col = 'date_1'\n",
            "num_days = 10\n",
            "ipdb> add_days(df,'date',20)\n",
            "+---------+----------+---+-----------+\n",
            "|person_id|      date|age|DATE_PLUS20|\n",
            "+---------+----------+---+-----------+\n",
            "|   id_001|2020-01-01| 52| 2020-01-21|\n",
            "|   id_002|2021-06-23| 63| 2021-07-13|\n",
            "|   id_003|2020-05-01| 16| 2020-05-21|\n",
            "+---------+----------+---+-----------+\n",
            "\n",
            "ipdb> continue\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/lib/python3.10/bdb.py\", line 347, in set_continue\n",
            "    sys.settrace(None)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pdb on"
      ],
      "metadata": {
        "id": "M5HrM0F-ML-N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}